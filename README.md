# Model Tester Project

This project tests and compares the documentation outputs generated by two locally installed models using the Ollama CLI. Instead of a single code snippet, it will process every Python file found in the `input_code/` directory. Each run’s results are saved as a timestamped Markdown file in the `results/` directory.

## Project Structure

- **main.py**  
  The entry point of the project. It scans the `input_code/` directory for Python files, loads each file's contents, runs tests using your locally installed models (via the Ollama CLI), and saves the outputs.

- **local_inference.py**  
  Contains functions that invoke the locally installed models via the Ollama CLI using Python’s subprocess module.

- **test_models.py**  
  Defines functions to run multiple models on a given code snippet (from each Python file) using a provided prompt.

- **evaluation.py**  
  Outlines evaluation criteria for comparing the generated documentation outputs.

- **input_code/**  
  Place your Python files here. Each file’s content will be used as input for documentation generation.

- **results/**  
  A folder where each run’s output is saved in a timestamped Markdown file.

## Setup Instructions

1. **Python Version:**  
   Ensure you have Python 3.7 or higher installed.

2. **Ollama CLI:**  
   Make sure the `ollama` command is available in your PATH and that your models are installed locally. Use `ollama list` to verify that, for example, you have models named `llama2:7b` and `llama3.2:latest`.

3. **Project Setup:**  
   - Create an `input_code/` directory in the project root and add your Python files to it.
   - Ensure the `results/` directory exists (it will be created automatically if not).

4. **Run the Project:**  
   From the project directory, run:
   ```bash
   python main.py
